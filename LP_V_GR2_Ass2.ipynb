{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5af7305",
      "metadata": {
        "id": "c5af7305"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fff83d32",
      "metadata": {
        "id": "fff83d32"
      },
      "outputs": [],
      "source": [
        "max([max(sequence) for sequence in train_data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41969f7c",
      "metadata": {
        "id": "41969f7c"
      },
      "outputs": [],
      "source": [
        "# word_index is a dictionary mapping words to an integer index\n",
        "word_index = imdb.get_word_index()\n",
        "# We reverse it, mapping integer indices to words\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "# We decode the review; note that our indices were offset by 3\n",
        "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
        "decoded_review = \" \".join([reverse_word_index.get(i - 3, \"?\") for i in train_data[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f344e908",
      "metadata": {
        "id": "f344e908"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.0  # set specific indices of results[i] to 1s\n",
        "    return results\n",
        "\n",
        "\n",
        "# Our vectorized training data\n",
        "x_train = vectorize_sequences(train_data)\n",
        "# Our vectorized test data\n",
        "x_test = vectorize_sequences(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85303041",
      "metadata": {
        "id": "85303041"
      },
      "outputs": [],
      "source": [
        "x_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3870313",
      "metadata": {
        "id": "d3870313"
      },
      "outputs": [],
      "source": [
        "# Our vectorized labels\n",
        "y_train = np.asarray(train_labels).astype(\"float32\")\n",
        "y_test = np.asarray(test_labels).astype(\"float32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e5de554",
      "metadata": {
        "id": "4e5de554"
      },
      "outputs": [],
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation=\"relu\", input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation=\"relu\"))\n",
        "model.add(layers.Dense(1, activation=\"sigmoid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a4e367f",
      "metadata": {
        "id": "3a4e367f"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "712b7497",
      "metadata": {
        "id": "712b7497"
      },
      "outputs": [],
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5228d771",
      "metadata": {
        "id": "5228d771"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    partial_x_train,\n",
        "    partial_y_train,\n",
        "    epochs=20,\n",
        "    batch_size=512,\n",
        "    validation_data=(x_val, y_val),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7c5fd53",
      "metadata": {
        "id": "d7c5fd53"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "861d959e",
      "metadata": {
        "id": "861d959e"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# Define a new review\n",
        "new_review = \"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\"\n",
        "\n",
        "\n",
        "# Preprocess the new review\n",
        "def preprocess_review(review, word_index, max_length=10000):\n",
        "    # Convert the review text to lowercase and tokenize it\n",
        "    words = review.lower().split()\n",
        "    # Convert words to indices using the word index dictionary\n",
        "    indices = [\n",
        "        word_index[word] if word in word_index and word_index[word] < 10000 else 0\n",
        "        for word in words\n",
        "    ]\n",
        "\n",
        "    indices = pad_sequences([indices], maxlen=max_length)\n",
        "    return indices\n",
        "\n",
        "\n",
        "# Preprocess the new review\n",
        "preprocessed_review = preprocess_review(new_review, word_index)\n",
        "\n",
        "# Use the model to predict the sentiment of the new review\n",
        "predicted_sentiment = model.predict(preprocessed_review)\n",
        "\n",
        "# Output the predicted sentiment\n",
        "print(\n",
        "    \"Predicted sentiment:\",\n",
        "    \"Positive\" if predicted_sentiment[0, 0] > 0.5 else \"Negative\",\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
